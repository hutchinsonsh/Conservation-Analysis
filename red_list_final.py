# -*- coding: utf-8 -*-
"""Red List Final.ipynb

Automatically generated by Colaboratory.

# Set up

NOTE: transfer from colab to py file not ideal --> hard to follow / a lot happening
First half: functions
Second half: calling functions 

"""

!pip install pyspark --quiet
!pip install -U -q PyDrive --quiet 
!apt install openjdk-8-jdk-headless &> /dev/null

# imports
from pyspark import SparkContext, SparkConf
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, count, countDistinct
from pyspark.sql.functions import *
from pyspark.sql import functions as F

#  spark session
spark = SparkSession \
    .builder \
    .appName("Red List app") \
    .getOrCreate()

# spark context
conf = SparkConf().set('spark.ui.port', '4050').setAppName("Red List app").setMaster("local[*]")
sc = SparkContext.getOrCreate(conf=conf)

"""# Import Data"""

# save data to original dataframes
filepath = "redList_final.csv"
redList_df = spark.read.csv(filepath, inferSchema=True, header=True, mode='DROPMALFORMED')
redList_df.show(10)

"""# Percentage Charts"""

import pandas as pd
import matplotlib.pyplot as plt

# Creates a pie graph based on species group distribution
# parameters = name of Listing Status, and Combine (ie- if # of species is less than x, add to 'other' total)
#   for clearning up graph (so there aren't groups w 1% clogging up graph)
from itertools import combinations_with_replacement
from pyparsing import Combine
def createPieSpecies(listingName, combine):
  # creates new sub dataframe based on threat level; 
  #   threat level can either be a list or a single status
  if type(listingName) == list: 
    total = redList_df.where(col('ESA Listing Status').isin(listingName)).count()
    tempCountList = redList_df.where(col('ESA Listing Status').isin(listingName)). \
      groupBy(col('Species Group')).agg({'*':'count'}).collect()
  elif type(listingName) == str: 
    total = redList_df.filter(col('ESA Listing Status') == listingName).count()
    tempCountList = redList_df.filter(col('ESA Listing Status') == listingName). \
      groupBy(col('Species Group')).agg({'*':'count'}).collect()

  # loop though tempList, find where count < combine, add to totalSum
  totalSum = 0
  for x in tempCountList:
    if x['count(1)'] <= combine: 
      totalSum += x['count(1)']

  # create pandas dataframe from tempCountList w/ removing rows that have a count of < combine
  percentPd = spark.createDataFrame(tempCountList).filter(col('count(1)') > combine).toPandas()
  if(totalSum > 0):
    percentPd.loc[len(percentPd.index)] = ['other', totalSum]
    #display(percentPd)
  
    # alterations
  plt.rc('font', size=12) 
  pie_colors = ['#3d5a80', '#98c1d9', '#e0fbfc', '#e7b4a5', '#ee6c4d']

  # actual graph
  plt.pie(percentPd["count(1)"], autopct='%1.0f%%', radius=1.5, pctdistance=.8, 
          labels=percentPd["Species Group"], colors=pie_colors)
  graphTitle = "total: " + str(total)
  if type(listingName) == str: 
    graphTitle = listingName.lower() + " total: " + str(total)
  plt.title(graphTitle,x=1.5, y=0)
  #plt.legend(bbox_to_anchor=(1.5,1), loc="upper left", labels=percentPd["Species Group"])

  fig1 = plt.gcf()
  plt.show()
  fileName = 'pie_graph.png'
  if type(listingName) == str: 
    fileName = "overall_pieGraph" + listingName + ".png"
  fig1.savefig(fileName, bbox_inches='tight')

# same thing as above but easier to just do small fixes than to combine the 
# two functions; makes a pie graph based on listing distributin
from itertools import combinations_with_replacement
from pyparsing import Combine
def createPieListing():
  showStatus = ['Endangered', 'Recovery', 'Threatened', 'Species of Concern']
  
  # creates new pandas df that's grouped by ESA listing status
  total = redList_df.count()
  tempRedList = redList_df.where(col('ESA Listing Status').isin(showStatus)). \
    groupBy(col('ESA Listing Status'))
  percentPd = tempRedList.agg({'*':'count'}).toPandas()
  

  # alterations
  plt.rc('font', size=12) 
  pie_colors = ['#3d5a80', '#98c1d9', '#e0fbfc', '#e7b4a5', '#ee6c4d']
  pie_colors.reverse()

  # actual graph
  plt.pie(percentPd["count(1)"], autopct='%1.0f%%', radius=1.5, pctdistance=.8, 
          colors=pie_colors)
  graphTitle = "ESA Listing Status Distribution\n" + " total: " + str(total)
  plt.title(graphTitle,x=1.5, y=0)
  plt.legend(bbox_to_anchor=(1.5,1), loc="upper left", labels=percentPd["ESA Listing Status"])

  fig1 = plt.gcf()
  plt.show()
  fileName = "listing_pieGraph" + ".png"
  fig1.savefig(fileName, bbox_inches='tight')

"""# Stemp Graphs: Added

For each listing, shows how many were added that year
"""

from pyspark.sql.functions import to_timestamp, date_format

def createStemGraph(status): 
  # time v species added for each group 
  # create a new dataframe w/ listing status/date/year only
  speciesPerYear_df = redList_df.filter(col('ESA Listing Status') == status). \
    withColumn("Date", to_date(col("ESA Listing Date"), 'M-d-y')). \
    withColumn("Year", date_format(col('Date'), "y")). \
    where(col('Date').isNotNull()). \
    select('ESA Listing Status', 'Date', 'Year'). \
    groupBy(col('Year')).count()
    #orderBy(['Year'], ascending = [True]). \ // leave out - later sorted in pandas dataframe

  # create pandas dataframe w/ columns year | count (# of species added that year)
  speciesPerYear_pdf = speciesPerYear_df.toPandas().sort_values(by='Year')

  # doesn't recognize year column as a 'date', need to fix in order to graph
  speciesPerYear_pdf['Year'] = pd.to_datetime(speciesPerYear_pdf['Year'], format='%Y')

  # plot graph w/ y = count, x = year; configurations
  title = "Number of Species Added Each Year for " + status.title() 
  font1 = {'color':'black','size':20}
  plt.figure(figsize=(25,10))
  plt.tick_params(axis='both',labelsize=14)
  plt.title(title, fontdict = font1)
  plt.xlabel("Year", fontdict = font1)
  plt.ylabel("Species Added", fontdict = font1)

  # save figure so that I can save it to a file
  fig1 = plt.gcf()

  # actual plotting
  plt.stem(speciesPerYear_pdf['Year'], speciesPerYear_pdf['count'])
  plt.show()

  # save to file
  fileName = "LineGraph_Added" + status + ".png"
  fig1.savefig(fileName, bbox_inches='tight')
createStemGraph('Threatened')

"""# Line Graphs: Culmination"""

def createLineGraph_cul(status): 
  # time v species added for each group 
  # create a new dataframe w/ listing status/date/year only
  # then turns that dataframe into a list w/ a collect statement
  # also ordered by year 
  speciesPerYear_ls = redList_df.filter(col('ESA Listing Status') == status). \
    withColumn("Date", to_date(col("ESA Listing Date"), 'M-d-y')). \
    withColumn("Year", date_format(col('Date'), "y")). \
    where(col('Date').isNotNull()). \
    select('ESA Listing Status', 'Date', 'Year'). \
    groupBy(col('Year')).count(). \
    orderBy(['Year'], ascending = [True]). \
    collect()

  # creates a new list where count column = # of species added cumulatively
  cumulatedCountList = []
  cumulatedCount = 0
  for x in speciesPerYear_ls: 
    cumulatedCountList.append((x[0], x[1] + cumulatedCount))
    cumulatedCount += x[1]

  # turn list into pandas dataframe
  speciesPerYear_pdf = pd.DataFrame (cumulatedCountList, columns = ['Year', 'Count'])

  # doesn't recognize year column as a 'date', need to fix in order to graph
  speciesPerYear_pdf['Year'] = pd.to_datetime(speciesPerYear_pdf['Year'], format='%Y')

  # plot graph w/ y = count, x = year; configurations
  title = "Cumulative Count of Species Added Each Year for status: " + status.title() 
  font1 = {'color':'black','size':20}
  plt.figure(figsize=(25,10))
  plt.tick_params(axis='both',labelsize=14)
  plt.title(title, fontdict = font1)
  plt.xlabel("Year", fontdict = font1)
  plt.ylabel("Species Added", fontdict = font1)

  # save figure so that I can save it to a file
  fig1 = plt.gcf()

  # actual plotting
  plt.plot(speciesPerYear_pdf['Year'], speciesPerYear_pdf['Count'])
  plt.show()

  fileName = "LineGraph_Cumulative" + status + ".png"
  fig1.savefig(fileName, bbox_inches='tight')

"""# Priority Based on Species"""

from pyspark.sql.functions import regexp_replace
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as snsy

# used to plot data created in createPriorityStacked
def plotPriorityGraph(spPriority_dic, status): 
  final = ['#003049', '#d62828', '#5f0f40', '#9a031e', '#985277', '#fb8b24', '#e36414', '#0f4c5c', 
          '#a4243b', '#5c374c', '#9dd9d2', '#392f5a', '#b4436c']
  final.reverse()
  # so this is actually plotting; for each plot, setup like : x, y, bottom
  # bottom is count of all elements before it; so if set1 = [1, 2, 3], set2 = [3, 5, 7]
  # set3 would have bottom of [4, 7, 10] (accumulation of elements before it)
  # this is the fastest way I could think to do it
  priority = [*range(1, 19)]
  bottom = []
  count = 0
  for (key, value) in spPriority_dic.items(): 
    if len(bottom) == 0: 
      plt.bar(priority, value, label=key, color=final[count])
      bottom = value
      count += 1
    else: 
      # will only plot if there's something to plot; otherwise unnecessary plotting/legend is really long
      allZeros = False
      for x in value: 
        if x != 0: 
          allZeros = True
      if(allZeros): 
        plt.bar(priority, value, bottom=bottom, label=key, color=final[count])
        for x in range(len(bottom)): 
          bottom[x] = bottom[x] + value[x]
        count +=1

  # actual plotting
  plt.xlabel('Priority Level')
  plt.ylabel('Number of Species')
  plt.title('Species Group Makeup of Each Priority Level: ' + status)
  plt.legend(bbox_to_anchor=(1,0), loc="lower left")
  plt.xticks(np.arange(1, 18, step=1))
  
  #show plot as image
  fig1 = plt.gcf()
  plt.show()
  fileName = "PriorityStackedGraph_" + status + ".png"
  fig1.savefig(fileName, bbox_inches='tight')

# TODO: histogram: how many of each priority are there in categories End, Threat, Recov
# TODO: makeup of each priority (which species are # 1 concern for each category)

# Steps: 
# 1: get priorities for each status listing
# 2: get species group for each priority
# 3: W/ species group, count, collect top 5 + count of all others
# 4: plot each count 

def createPriorityStacked(status): 
  # create a new dataframe w/ for a status w/ priority number and species group columns
  redListPriority_df = redList_df.filter(col('ESA Listing Status') == status). \
    where(col('Recovery Priority Number').isNotNull()). \
    where(col('Recovery Priority Number') != 'UNK'). \
    withColumn("RPN_noC", regexp_replace('Recovery Priority Number', 'C', '')). \
    select('Species Group', 'RPN_noC')

  # create a dictionary: {speciesGroup: [count for each priority], }
  # for each list for a speciesGroup, has 18 entries, w/ 0 if they don't make top 5
  # this creates the outline of a dictionary w/ setup like speciesGroup : list of count
  speciesTypes_ls = redList_df.select('Species Group').distinct().collect()
  spPriority_dic = {speciesTypes_ls[i][0]: [] for i in range(0, len(speciesTypes_ls))}
  spPriority_dic['Other'] = []

  # for every priority level, get species count w/in that level
  # if in top 5, adds to tempList. Otherwise
  for currentPriority in range(1, 19): 
    topFive = {}    # {species:count}
    # returns a list w/ row()
    countOfSpecies_ls = redListPriority_df.filter(col('RPN_noC') == currentPriority). \
      groupBy(col('Species Group')).count().alias('count').\
      orderBy(['count'], ascending = [False]).collect()

    # finds top
    otherCount = 0
    top5_ls = countOfSpecies_ls[:4]
    cut5_ls = countOfSpecies_ls[5:]

    # hard to explain my logic; For each key, needs 19 values- 
    #   either it's actual count or 0 (since it is counted w/ others)
    #   so I add top5 actual values to dic, count up 'other' count, and add 0 to cut species
    #   end result: every key will have 19 values --> use those to make a stacked plot
    for x in top5_ls: 
      spPriority_dic[x[0]].append(x[1])
    for x in cut5_ls: 
      otherCount += x[1]
    spPriority_dic['Other'].append(otherCount)
    for (k, v) in spPriority_dic.items(): 
      if(len(v) < currentPriority): 
        v.append(0)
  plotPriorityGraph(spPriority_dic, status)

"""# Mapping: Basic"""

!pip install geopandas
!pip install mapclassify
!pip install contextily     # for map overlay
# !pip install geoplot

import geopandas as gpd
import pandas as pd
from pathlib import Path
import folium
import matplotlib.pyplot as plt
from mapclassify import classify
import contextily as cx
from pyspark.sql.types import IntegerType

# major function that, for each list, reads shapefile url and creates a new file
# returns count of good/bad links and a list of geoDataFrames 
def readShapefile(status): 
  # for holding all shapefile lists
  shapefiles_ls = []
  # creates a list of shapefiles for all rows w/ specified status
  status_ls = redList_df.filter(col('ESA Listing Status') == status). \
    where(col('Range Shapefile_url').isNotNull()). \
    where(col('Recovery Priority Number') != 'UNK'). \
    withColumn("Priority1", regexp_replace('Recovery Priority Number', 'C', '')). \
    withColumn("Priority", col('Priority1').cast(IntegerType())) .\
    where(col('Foreign or Domestic') == 'Domestic'). \
    orderBy(col('Priority')). \
    select(col('Range Shapefile_url'), col('Species Group'), col('Priority')).collect()

  badLinks = redList_df.filter(col('ESA Listing Status') == status). \
    where(col('Range Shapefile_url').isNull()).count()
  goodLinks = 0

  # for each row, tries to read shapefile url, adds species column (for later mapping)
  #  note: each shapefile includes metadata (species name, etc) and is a list- not just coordinates
  for x in status_ls: 
        try: 
          # create new geoDataFrame; add column for species group (used for mapping later)
          temp = gpd.read_file(x[0])
          temp['SpeciesGroup'] = x[1]
          temp['Priority'] = x[2]
          shapefiles_ls.append(temp)
          goodLinks += 1
          # takes a long time, this is to check how far the progress is
          if(goodLinks % 100 == 0): 
            print("{} files read".format(goodLinks))
        except:
          badLinks += 1

  return badLinks, goodLinks, shapefiles_ls

def createMap(geoDF_ls, status, choice): 
  world = geoDF_ls[0]
  for specieSF in range(1, len(geoDF_ls)):
    if(status == 'Endangered'): 
      if(specieSF != 98 and specieSF != 405 and specieSF != 1059):
        world = world.append(geoDF_ls[specieSF]) 
    else:
      world = world.append(geoDF_ls[specieSF]) 

  # changes the xlim, otherwise looks very off
  #plt.xlim([-130, -60])
  #plt.ylim([20, 55])
  fig, ax= plt.subplots(figsize=(30,18))

  #fig, ax = plt.subplots(figsize=(10, 10))
  fig1 = plt.gcf()
  df_wm = world.to_crs(epsg=3857)
  ax2 = df_wm.plot(column=choice, cmap='inferno', legend=True, figsize=(10, 10), alpha=0.5, ax=ax)
  cx.add_basemap(ax2, crs='EPSG:3857', source=cx.providers.Stamen.TonerLite)
  ax2.set_ylim(.3*1e7, .7*1e7)
  ax2.set_xlim(-1.6*1e7, -.5*1e7)

  fileToSave = "{}{}Map.png".format(status, choice)
  fig.savefig(fileToSave)

# FOR TESTING: 
# HOW I FOUND WHICH DATAPOINTS WERE BAD
# absolutly embaressing code --> never look at or think about again
def dontCall(): 

  start = 1050
  end = 1070
  actualEnd = 1071
  while end < actualEnd:
    # geoDF_ls was just a dataframe of shapefiles 
    world = geoDF_ls[start] 
    for specieSF in range(start, end):
      if(specieSF != 98 and specieSF != 405 and specieSF != 1059): 
        world = world.append(geoDF_ls[specieSF]) 
    fig, ax= plt.subplots(figsize=(30,18))

    #fig, ax = plt.subplots(figsize=(10, 10))
    fig1 = plt.gcf()
    df_wm = world.to_crs(epsg=3857)
    ax2 = df_wm.plot(column='SpeciesGroup', cmap='inferno', legend=True, figsize=(10, 10), alpha=0.5, ax=ax)
    cx.add_basemap(ax2, crs='EPSG:3857', source=cx.providers.Stamen.TonerLite)

    fileToSave = 'Dstatus' + str(start) + "Map.png"
    fig.savefig(fileToSave) 
    start = start+1
    end = end+10000

  # item1:

"""# Area of each shapefile"""

# creates a pie graph for each area 
def createAreaPie(shapefile_ls, status):
  # for each geoDataFrame, get area, priority, and species 
  #  --> add as an entry in a list
  areasForRecover = []
  for x in shapefile_ls: 
    #area = x.area()
    x = x.to_crs(3857)
    area = x.geometry.area
    area = float(area[0])
    species = x.SpeciesGroup
    #priority = x.Priority
    areaGroup = 0

    # redwoods:
    if area < 6054812557.2: 
      areaGroup = 1
    # hawaii valcanos: 
    elif area < 14088670913: 
      areaGroup = 2
    # yosemite
    elif area < 33181721100: 
      areaGroup = 3
    # size of grand canyon
    elif area < 53024716800:  #11
      areaGroup = 4
    # yellowstone
    elif area < 96765926400:  #11
      areaGroup = 5
    # size of maricopa county
    elif area < 257150361600:  #12
      areaGroup = 6
    else: 
      areaGroup = 7
    areasForRecover.append([area, species[0], areaGroup])

  
  # create new dataframe from list created above
  columns = ['area', 'species', 'areaGroup']
  area_df = spark.createDataFrame(areasForRecover, columns)

  # create new grouped dataframe of each area  --> have areaGroup | count (so 6 rows)
  total = area_df.count()
  tempCountList = area_df.orderBy(col('areaGroup')).groupBy(col('areaGroup')).agg({'*':'count'}).collect()

  # create pandas dataframe based on altered area_df
  percentPd =  spark.createDataFrame(tempCountList).toPandas()
  
    # alterations
  plt.rc('font', size=12) 
  pie_colors = ['#3d5a80', '#98c1d9', '#e0fbfc', '#e7b4a5', '#ee6c4d']

  # actual graph
  plt.bar(percentPd['areaGroup'], percentPd["count(1)"])
  graphTitle = "Distribution of Area Sizes for Listing: " + status
  plt.xlabel("Area Sizes")
  plt.ylabel("Number of Species")
  plt.xticks(np.arange(1, 6, step=1))
  plt.title(graphTitle)

  fig1 = plt.gcf()
  plt.show()
  fileName = "area_pieGraph" + status + ".png"
  fig1.savefig(fileName, bbox_inches='tight')

# decided not to go w/ --> too wide an area (size of grand canyon to Africa --> no good way to show)
def createPriorityHist(shapefile_ls, status):
  # for each geoDataFrame, get area, priority, and species 
  #  --> add as an entry in a list
  areasForRecover = []
  for x in shapefile_ls: 
    x = x.to_crs(3857)
    area = x.geometry.area
    area = float(area[0])
    species = x.SpeciesGroup
    priority = x.Priority
    areasForRecover.append([area, species[0], priority[0]])

  # create new dataframe from list created above
  columns = ['area', 'species', 'priority']
  area_pdf = pd.DataFrame(areasForRecover, columns=columns)

  # plot dataframe in a histogram on 'Area' column
  title = "Area of Species Distribution: " + status.title() 
  plt.title(title)
  plt.xlabel("Area (in Hectare)")
  plt.ylabel("Number of Species")
 
  if status == 'Endangered': 
    plt.hist(area_pdf['area'], bins=50, edgecolor="black", range=[0*1e12, 5*1e7])
  elif status == 'Threatened': 
    plt.hist(area_pdf['area'], bins=50, edgecolor="black", range=[0*1e13, 1*1e8])
  else: 
    plt.hist(area_pdf['area'], bins=50, edgecolor="black")
  fig1 = plt.gcf()
  plt.show()
  fileName = "histogram_" + status + ".png"
  fig1.savefig(fileName, bbox_inches='tight')

"""# Function Calls

Pie graph for listing distribution
"""

createPieListing()

"""Pie graph for species distribution"""

# individual
listings = {'Endangered':10, 'Threatened':10, 'Species of Concern':25, 'Extinction':0, 'Recovery':0}
for (k, v) in listings.items(): 
  createPieSpecies(k, v)

# combine end, threat, SoC
createPieSpecies(['Endangered', 'Threatened', 'Species of Concern'], 60)

"""Stem Years Added"""

listings = ['Endangered', 'Threatened', 'Extinction', 'Recovery']
for x in listings: 
  createStemGraph(x)

"""Line: Culmination of species added"""

listings = ['Endangered', 'Threatened', 'Extinction', 'Recovery']
for x in listings: 
  createLineGraph_cul(x)

"""Stacked Plot: for Species per Priority"""

statusList = ['Endangered', 'Threatened']
for x in statusList: 
  createPriorityStacked(x)

"""Maps: Distribution of Species"""

print(redList_df.where(col('Foreign or Domestic') == 'Domestic').where(col('ESA Listing Status') == 'Endangered').count())
print(redList_df.where(col('Foreign or Domestic') == 'Foreign').where(col('ESA Listing Status') == 'Endangered').count())
print(redList_df.where(col('Foreign or Domestic') != 'Domestic').where(col('ESA Listing Status') == 'Endangered').count())

# TODO: add in map label and good/bad links
# TODO: fix endangered 
shapefiles = []
good = []
bad = []
listings = ['Endangered', 'Threatened', 'Extinction', 'Recovery']
for x in listings: 
  badLinks, goodLinks, shapefile_ls = readShapefile(x)
  shapefiles.append(shapefile_ls)
  good.append(goodLinks)
  bad.append(badLinks)

# map based on species
for x in range(0, 2): 
  createMap(shapefiles[x], listings[x], 'SpeciesGroup')

# area of each shapefile
for x in range(2): 
  if(len(shapefiles[x]) != 0): 
    createAreaPie(shapefiles[x], listings[x])
  else: 
    print("{} is 0".format(listings[x]))
